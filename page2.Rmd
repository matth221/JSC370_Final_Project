---
title: "Impact of Smoking, Alcohol Consumption, and Happiness on Life Expectancy Trend in the World"
author: "Christopher Matthew"
date: "4/21/2022"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

# Methods

## Data Collection

I selected these variables based on some background research that I believe that the variables will have a high correlation with life expectancy in general. The initial regression model contains life expectancy at birth as the response variable while the predictor variables include smoking, alcohol consumption, and happiness.

Life expectancy at birth, denoted as life_expectancy_at_birth reflects the overall mortality level of a population. This represents the average number of years that a newborn is expected to live based on current mortality rates. This data is collected from https://data.worldbank.org/indicator/SP.DYN.LE00.IN

Smoking rates by country, denoted as smokepercentage, is defined as the proportion of
population over the minimum smoking age that smokes regularly in the country is one of the
well-known causes of reduction in life expectancy (Manuel, D. G., 2012, p.1 and Janssen et al.,
2021). This data is collected from https://worldpopulationreview.com/country-rankings/smoking-rates-by-country

Alcohol consumption per capita, denoted as alcohol consumption, is defined as number of
pure alcohol litres consumed every year per capita over legal drinking age also increases
mortality and cancer risk in older adults (Kunzmann et al., 2018). This data is collected from https://data.worldbank.org/indicator/SH.ALC.PCAP.LI

World happiness index, denoted as happiness, is defined by average answers to the main life evaluation question known as Cantril ladder, which asks respondents to think of a ladder with the best possible life for them being a 10 and worst possible life for them being a 0 and rate themselves. This data is collected from https://www.kaggle.com/datasets/unsdsn/world-happiness

Each of these datasets are taken from year 2018 for consistency. We will also use a Country-Continent Dataset to group each of the countries based on their continents.

## Data Wrangling

To begin with, I imported several libraries that I am going to use for the data analysis which includes tidyverse, dplyr, mgcv, car, gridExtra, plotly, and leaflet. After collecting the data from the sources containing similar time stamp, I used read.csv method to save the dataset into the environment data. The datasets are labeled as smoking, alcohol, happiness, continent, and life_expectancy. Then, for each of the dataset, I only selected the country column and the value index column for smokepercentage, alcohol_consumption, happiness and life_expectancy_at_birth respectively, all from year 2018 for consistency, and remove other unnecessary columns. This is done with the help of subset and colnames function from the base R package.

Next, for each of the datasets, I removed any missing values because imputing the values with median or mean is not suitable given that our information is limited. Next, I remove outliers for each of the datasets using my own defined outliers function which removes anything above or below the upper or lower quartile ± 1.5 Interquartile Range. In the end, I combined all of the datasets into a combined dataset grouped by each country using the group_by function and remove any countries that does not contain all three information. During each step, I also make sure that everything is going according to the plan by looking at the summaries and environment tab.

## Linear Modelling

```{r, echo = FALSE, message = FALSE, warning = FALSE}
outliers <- function(x) {

  Q1 <- quantile(x, probs=.25)
  Q3 <- quantile(x, probs=.75)
  iqr = Q3-Q1

 upper_limit = Q3 + (iqr*1.5)
 lower_limit = Q1 - (iqr*1.5)

 x > upper_limit | x < lower_limit
}

remove_outliers <- function(df, cols = names(df)) {
  for (col in cols) {
    df <- df[!outliers(df[[col]]),]
  }
  df
}
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
smoking <- read.csv(file = "data/Smoking.csv", header = TRUE)
alcohol <- read.csv(file = "data/Alcohol Consumption.csv", header = TRUE)
life_expectancy <- read.csv(file = "data/Life Expectancy.csv", header = TRUE)
happiness <- read.csv(file = "data/Happiness.csv", header = TRUE)
continent <- read.csv(file = "data/Continents.csv", header = TRUE)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
smoking <- na.omit(subset(smoking, select = c(1, 2)))
colnames(smoking) <- c("Country", "smokepercentage")
alcohol <- na.omit(subset(alcohol, select = c(1, 63)))
colnames(alcohol) <- c("Country", "alcohol_consumption")
life_expectancy <- na.omit(subset(life_expectancy, select = c(1, 63)))
colnames(life_expectancy) <- c("Country", "life_expectancy_at_birth")
happiness <- na.omit(subset(happiness, select = c(2, 3)))
colnames(happiness) <- c("Country", "happiness")
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
summary(smoking)
summary(alcohol)
summary(life_expectancy)
summary(happiness)
summary(continent)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
smoking <- remove_outliers(smoking, c('smokepercentage'))
alcohol <- remove_outliers(alcohol, c('alcohol_consumption'))
life_expectancy <- remove_outliers(life_expectancy, c('life_expectancy_at_birth'))
happiness <- remove_outliers(happiness, c('happiness'))
combined_dataset <- na.omit(merge(merge(merge(smoking, alcohol, by = 'Country'), happiness, by = 'Country'), life_expectancy, by = 'Country'))
combined_dataset <- na.omit(merge(combined_dataset, continent, by = 'Country'))
```

Afterwards, I am going to perform EDA by checking scatterplot between response and predictor and creating boxplots and histograms of the variables. I will also check for any nonlinear trend, clusters, discernible patterns, and normality of distribution of residuals to verify linear model assumptions.

First, I will take a look into the linearity of each pair of independent and dependent variables using the scatterplots to see if linearity seems to be present or not. Next, I am going to check normality using histogram, QQ-Plot, and a goodness of fit test. In case the data is not normally distributed, I will use boxcox power transformation to generate a better model regarding the normality assumption and see if the problem will be resolved or not. Subsequently, I will check for multicollinearity by looking at the VIF for the predictors and report any detrimental multicollinearity if they exist. Conclusively, I will check for homoscedasticity and also do appropriate adjustments if needed.

After fulfilling model assumptions,
we will check all possible models (1, 2, or 3 predictors) to evaluate their significance and Adjusted R^2 and conduct partial F-test to find the best model. Note that since there are only 7       
non-empty distinct subsets of the predictors, we will just manually check all three since using
other selection methods like AIC or automated selection is inefficient.

In the end, we will proceed with fitting a linear model to see our result and performance of the linear model and compare it with advanced regression model with cubic regression splines on each non-empty distinct subsets of the predictor variables.

## Machine Learning

We Will also try to fit Regression Tree, Bagging, and Random Forest with our dataset and calculate each of the test MSE for each of the method. We will then compare them to find which model fits the best for our dataset.

# Results

## Linear Modelling

After the initial data cleaning, we have 112 observations in our combined dataset, none of which contains any missing values. Stepping into the Exploratory Data Analysis, Figure 1 shows that the distribution for smoking is like normal distribution while alcohol and life expectancy have a right skewed and left skewed non-normal distribution respectively. The value for each variable seems to have a big variance and there aren’t any noticeable extreme observations.

```{r, fig.height = 8, fig.width = 8, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Boxplots and Histograms of Percentage of Smokers, Average Alcohol Consumption, and Life Expectancy at Birth"}
plot1 <- ggplot(combined_dataset, aes(smokepercentage)) + geom_histogram(color = "red", fill = "blue") + ggtitle("Histogram of Smoking") + xlab("Percentage of Smokers") + ylab( "Frequency")
plot2 <- ggplot(combined_dataset, aes(alcohol_consumption)) + geom_histogram(color = "red", fill = "blue") + ggtitle("Histogram of Alcohol") + xlab("Average Alcohol Consumption (Pure Litres per Year)") + ylab("Frequency")
plot3 <- ggplot(combined_dataset, aes(life_expectancy_at_birth)) + geom_histogram(color = "red", fill = "blue") + ggtitle("Histogram of Life Expectancy") + xlab("Life Expectancy at Birth") + ylab( "Frequency")
plot4 <- ggplot(combined_dataset, aes(happiness)) + geom_histogram(color = "red", fill = "blue") + ggtitle("Histogram of Happiness") + xlab("Happiness Index") + ylab( "Frequency")
grid.arrange(plot1, plot2, plot3, plot4, ncol = 2)
```

Next, Figure 2 shows that linearity assumption between each pair of independent variables and dependent variable other than Happiness Index is not very strong and we have to note this as our final result might not be accurate.

```{r, fig.height = 4, fig.width = 12, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Scatterplots between Predictor Variables and Response Variable"}
par(mfrow=c(1,3))
plot(y = combined_dataset$smokepercentage, x = combined_dataset$life_expectancy_at_birth, ylab = "Percentage of Smokers", xlab = "Life Expectancy at Birth", main = "Percentage of Smokers Vs. Life Expectancy at Birth")
plot(y = combined_dataset$alcohol_consumption, x = combined_dataset$life_expectancy_at_birth, ylab = "Alcohol Consumption per Capita per Year (pure litres per year)", xlab = "Life Expectancy at Birth", main = "Alcohol Consumption Vs. Life Expectancy at Birth")
plot(y = combined_dataset$happiness, x = combined_dataset$life_expectancy_at_birth, ylab = "Happiness Index", xlab = "Life Expectancy at Birth", main = "Happiness Index Vs. Life Expectancy at Birth")
```

Now, looking to Figure 3, we can see that the fitted against the real value seems to have a good
linear relationship and the pattern in the residual plot have slight fanning which shows a problem
in non-constant variance and there also exists some observations far from zero. Q-Q plot shows
that normality seems to be fulfilled and it appears that smoking and alcohol also have a slight
linear relationship which will be covered later in multicollinearity.

```{r, fig.height = 6, fig.width = 9, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Fitted and Residual Plots, QQ-Plot, Predictors Plot Before Transformation"}
model = lm(life_expectancy_at_birth ~ smokepercentage + alcohol_consumption + happiness, data = combined_dataset)
# summary(model)
# confint(model, level=0.95)
residual = resid(model)
residual <- resid(model)
par(mfrow=c(2,3))
plot(life_expectancy_at_birth ~ fitted(model), main="Fitted vs Life Expectancy", xlab="Life Expectancy at Birth", ylab="Fitted", data=combined_dataset)
abline(a = 0, b = 1)
plot(residual ~ fitted(model), main="Residuals vs Fitted", xlab="Fitted", ylab="Residual", data=combined_dataset)
plot(residual ~ smokepercentage, main="Residuals vs Smoking", xlab="Percentage of Smokers", ylab="Residual", data=combined_dataset)
plot(residual ~ alcohol_consumption, main="Residuals vs Alcohol", xlab="Average Alcohol Consumption", ylab="Residual", data=combined_dataset)
plot(residual ~ happiness, main="Residuals vs Happiness", xlab="Happiness Index", ylab="Residual", data=combined_dataset)
qqline(residual)
qqnorm(residual)
```
```{r, echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
vif(model)
boxCox(model)
p <- powerTransform(cbind(combined_dataset[,2], combined_dataset[,3], combined_dataset[,4], combined_dataset[,5]))
summary(p)
```

Based on our observation, we will do a power transformation to the variables, and it shows that
we must increase the exponent of our response variable to the power of five. After
transformation, we can see that in Figure 3, our residual plots seem to have a better pattern in
homoscedasticity and a slightly better QQ-Plot. Our fitted value also has a better linear
relationship with the actual value compared to the previous chart. Note that the VIF between the
predictors are below 1.3 which is a very good indicator that there is no multicollinearity between
the predictors.

```{r, fig.height = 6, fig.width = 9, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Fitted and Residual Plots, QQ-Plot, Predictors Plot After BoxCox Transformation"}
model1 = lm(life_expectancy_at_birth^5 ~ smokepercentage + alcohol_consumption + happiness, data = combined_dataset)
# summary(model)
# confint(model, level=0.95)
residual1 = resid(model1)
residual1 <- resid(model1)
par(mfrow=c(2,3))
plot(life_expectancy_at_birth^5 ~ fitted(model1), main="Fitted vs Life Expectancy^5", xlab="Life Expectancy at Birth^5", ylab="Fitted", data=combined_dataset)
abline(a = 0, b = 1)
plot(residual1 ~ fitted(model1), main="Residuals vs Fitted", xlab="Fitted", ylab="Residual", data=combined_dataset)
plot(residual1 ~ smokepercentage, main="Residuals vs Smoking", xlab="Percentage of Smokers", ylab="Residual", data=combined_dataset)
plot(residual1 ~ alcohol_consumption, main="Residuals vs Alcohol", xlab="Average Alcohol Consumption", ylab="Residual", data=combined_dataset)
plot(residual1 ~ happiness, main="Residuals vs Happiness", xlab="Happiness Index", ylab="Residual", data=combined_dataset)
qqline(residual1)
qqnorm(residual1)
```

Now that our assumptions have been fulfilled, on Appendix 1, we run different models
with all non-empty subsets of the predictors to find out that the model with both predictors have
a low adjusted R^2 of 0.6964 which is better than other models as you can see from the table below. Hence, we are going to proceed with the model containing 3 predictors.

\newpage

```{r, fig.cap= "Adjusted R^2 for Linear Models With Different Predictors"}
tibble(Model = c('All Three Predictors', 'Percentage of Smokers and Average Alcohol Consumption', 'Average Alcohol Consumption and Happiness', 'Percentage of Smokers and Happiness', 'Percentage of Smokers only', 'Average Alcohol Consumption only', 'Happiness only'),
       `Adjusted R-Squared` = c(0.696, 0.210, 0.672, 0.693, 0.063, 0.191, 0.662)) %>%
       knitr::kable(caption = "Adjusted R^2 for Linear Models with Different Predictors")
```


Following our model selection, we fit multiple models with cubic regression spline on the predictors in hope for a better model. However, based on Table 2 and Appendix 2, we can see that there isn't any significant jump in performance between the models. Each of the resulting model have an Adjusted R^2 of 0.696. Hence, we will stick with our original linear model with all the predictors without any splines.

```{r, fig.cap= "Adjusted R^2 for Linear Models With/Without Cubic Regression Splines on the Predictors"}
tibble(Model = c('No Splines', 'Percentage of Smokers only', 'Average Alcohol Consumption only', 'Happiness only'),
       `Adjusted R-Squared` = c(0.6964, 0.6961, 0.6963, 0.6962)) %>%
       knitr::kable(caption = "Adjusted R^2 for Linear Models With/Without Cubic Regression Splines on the Predictors")
```

## Machine Learning

From the figure below, we obtain the Regression Tree before Optimal CP Pruning and after Optimal CP Pruning. Since our dataset is small, the cutoff is generally very close to the root which causes the height of the tree after pruning to become 2.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Regression Tree before Optimal CP Pruning"}
set.seed(314)
n <- floor(0.7 * nrow(combined_dataset))
sample <- sample.int(n = nrow(combined_dataset), size = floor(.7*nrow(combined_dataset)), replace = F)
train <- combined_dataset[sample,]
test <- combined_dataset[-sample,]
treefit <- rpart(life_expectancy_at_birth ~ smokepercentage + alcohol_consumption + happiness, method = "anova", control = list(cp=0), data = train)
rpart.plot(treefit)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, include = FALSE}
plotcp(treefit)
printcp(treefit)
optimalcp <- 0.15176
treepruned <- prune(treefit, cp = optimalcp)
summary(treepruned)
```



```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Regression Tree after Optimal CP Pruning"}
rpart.plot(treepruned)
```

As expected, the test MSE for regression tree is higher than our original linear model since we only use a single categorical divider into a continuous variable which causes the data to be splitted very unevenly which causes higher average MSE.

```{r, echo = FALSE, message = FALSE, warning = FALSE, include = F}
tree_pred <- predict(treepruned, test)
test_tree <- cbind(test, tree_pred)

tree_mse <- sum((test_tree$tree_pred - test_tree$life_expectancy_at_birth)^2)/dim(test_tree)[1]
tree_mse

lm_pred <- predict(model, test)
test_lm <- cbind(test, lm_pred)

lm_mse <- sum((test_lm$lm_pred - test_lm$life_expectancy_at_birth)^2)/31
lm_mse
```

Similarly, for our bagging and random forest, we obtain that happiness is the most important factor and since there only exists 3 different features, the test MSE for these methods are also bigger than the MSE for our original linear model. We will show the figures in detail in the following summary.

```{r, echo=FALSE, eval=FALSE, warning=FALSE}
bag <- randomForest(life_expectancy_at_birth ~ smokepercentage + alcohol_consumption + happiness, data = train, mtry=3, na.action = na.omit)
varImpPlot(bag, n.var = 3, col = "red")
```
```{r, echo=FALSE, eval=FALSE, warning=FALSE, include = F}
rf <- randomForest(life_expectancy_at_birth ~ smokepercentage + alcohol_consumption + happiness, data = train, na.action = na.omit)
varImpPlot(rf, n.var = 3, col = "red")

bag_pred <- predict(bag, test)

bag_tree <- cbind(test, bag_pred)

bag_mse <- mean((bag_pred - test$life_expectancy_at_birth)^2)

bag_mse

rf_pred <- predict(rf, test)

rf_tree <- cbind(test, rf_pred)

rf_mse <- mean((rf_pred - test$life_expectancy_at_birth)^2)

rf_mse
```

## Data Exploration {.tabset}

### 3D Scatterplot Between the Predictors Per Country
```{r, warning = FALSE, message = FALSE}
combined_dataset %>% 
  plot_ly(x = ~smokepercentage, y = ~alcohol_consumption, z = ~happiness,
          type = 'scatter3d', mode = 'markers', color = ~Country,
          hoverinfo = 'text',
          text = ~paste( paste(Country, ":", sep=""), paste(" Smoking Rates: ", smokepercentage, sep="") , paste(" Average Alcohol Consumption (Pure Litres Per Year): ", 
                        alcohol_consumption, sep=""), paste(" Happiness Index: ", happiness, sep=""), sep = "<br>")) %>% 
  layout(title = "Smoking Rates, Average Alcohol Consumption, and Happiness Around the World",
         yaxis = list(title = "Average Alcohol Consumption (Pure Litres Per Year)"), xaxis = list(title = "Smoking Rates"), 
         hovermode = "compare")
```

### 3D Scatterplot Between the Predictors Per Continent
```{r, warning = FALSE, message = FALSE}
combined_dataset %>% 
  plot_ly(x = ~smokepercentage, y = ~alcohol_consumption, z = ~happiness,
          type = 'scatter3d', mode = 'markers', color = ~Continent,
          hoverinfo = 'text',
          text = ~paste( paste(Country, ":", sep=""), paste(" Smoking Rates: ", smokepercentage, sep="") , paste(" Average Alcohol Consumption (Pure Litres Per Year): ", 
                        alcohol_consumption, sep=""), paste(" Happiness Index: ", happiness, sep=""), sep = "<br>")) %>% 
  layout(title = "Smoking Rates, Average Alcohol Consumption, and Happiness Around the World",
         yaxis = list(title = "Average Alcohol Consumption (Pure Litres Per Year)"), xaxis = list(title = "Smoking Rates"), 
         hovermode = "compare")
```

### 3D Scatterplot + Size for Life Expectancy Per Country
```{r, warning = FALSE, message = FALSE}
combined_dataset %>% 
  plot_ly(x = ~smokepercentage, y = ~alcohol_consumption, z = ~happiness,
          type = 'scatter3d', mode = 'markers', color = ~Country,
          size = ~life_expectancy_at_birth, sizes = c(5, 30), marker = list(sizemode='diameter', opacity=0.5),
          hoverinfo = 'text',
          text = ~paste( paste(Country, ":", sep=""), paste(" Smoking Rates: ", smokepercentage, sep="") , paste(" Average Alcohol Consumption (Pure Litres Per Year): ", 
                        alcohol_consumption, sep=""), paste(" Happiness Index: ", happiness, sep=""), sep = "<br>")) %>% 
  layout(title = "Smoking Rates, Average Alcohol Consumption, and Happiness Around the World",
         yaxis = list(title = "Average Alcohol Consumption (Pure Litres Per Year)"), xaxis = list(title = "Smoking Rates"), 
         hovermode = "compare")
```

### 3D Scatterplot + Size for Life Expectancy Per Continent
```{r, warning = FALSE, message = FALSE}
combined_dataset %>% 
  plot_ly(x = ~smokepercentage, y = ~alcohol_consumption, z = ~happiness,
          type = 'scatter3d', mode = 'markers', color = ~Continent,
          size = ~life_expectancy_at_birth, sizes = c(5, 30), marker = list(sizemode='diameter', opacity=0.5),
          hoverinfo = 'text',
          text = ~paste( paste(Country, ":", sep=""), paste(" Smoking Rates: ", smokepercentage, sep="") , paste(" Average Alcohol Consumption (Pure Litres Per Year): ", 
                        alcohol_consumption, sep=""), paste(" Happiness Index: ", happiness, sep=""), sep = "<br>")) %>% 
  layout(title = "Smoking Rates, Average Alcohol Consumption, and Happiness Around the World",
         yaxis = list(title = "Average Alcohol Consumption (Pure Litres Per Year)"), xaxis = list(title = "Smoking Rates"), 
         hovermode = "compare")
```

\newpage

## Description {.tabset}

Description for each of the visualizations above.

### 3D Scatterplot Between the Predictors Per Country

This plot shows the value of each predictor variable for every country which includes the smoking rates, average alcohol consumption (Pure Litres Per Year), and happiness index.

### 3D Scatterplot Between the Predictors Per Continent

This plot shows the value of each predictor variable for every continent which includes the smoking rates, average alcohol consumption (Pure Litres Per Year), and happiness index.

### 3D Scatterplot + Size for Life Expectancy Per Country

This plot shows the value of smoking rates, average alcohol consumption (Pure Litres Per Year), and happiness index for every country and each point size is represented by the life expectancy at birth of the country. Generally, we want to see that the country with lower smoking rates, average alcohol consumption, and happiness index will have a bigger circle radius.

### 3D Scatterplot + Size for Life Expectancy Per Continent

This plot shows the value of smoking rates, average alcohol consumption (Pure Litres Per Year), and happiness index for every continent and each point size is represented by the life expectancy at birth of the continent. Generally, we want to see that the continent with lower smoking rates, average alcohol consumption, and happiness index will have a bigger circle radius.